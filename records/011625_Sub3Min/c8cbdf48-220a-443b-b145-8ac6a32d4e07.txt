import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # Set attention scale such that the minimum attainable attention entropy
        # (but not necessary the attention entropy itself) is close to 0. By @leloykun
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1385 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan 17 2025, 03:57:17) [GCC 13.2.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Fri Jan 17 05:37:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   34C    P0            121W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            125W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1385 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1385 train_time:17555ms step_avg:nanms
step:2/1385 train_time:18326ms step_avg:nanms
step:3/1385 train_time:18444ms step_avg:nanms
step:4/1385 train_time:18565ms step_avg:nanms
step:5/1385 train_time:18687ms step_avg:nanms
step:6/1385 train_time:18809ms step_avg:nanms
step:7/1385 train_time:18931ms step_avg:nanms
step:8/1385 train_time:19053ms step_avg:nanms
step:9/1385 train_time:19175ms step_avg:nanms
step:10/1385 train_time:19300ms step_avg:nanms
step:11/1385 train_time:126ms step_avg:nanms
step:12/1385 train_time:249ms step_avg:nanms
step:13/1385 train_time:372ms step_avg:124.01ms
step:14/1385 train_time:495ms step_avg:123.63ms
step:15/1385 train_time:616ms step_avg:123.24ms
step:16/1385 train_time:738ms step_avg:123.08ms
step:17/1385 train_time:861ms step_avg:122.99ms
step:18/1385 train_time:984ms step_avg:122.96ms
step:19/1385 train_time:1107ms step_avg:123.05ms
step:20/1385 train_time:1230ms step_avg:123.02ms
step:21/1385 train_time:1352ms step_avg:122.95ms
step:22/1385 train_time:1476ms step_avg:122.99ms
step:23/1385 train_time:1598ms step_avg:122.89ms
step:24/1385 train_time:1720ms step_avg:122.89ms
step:25/1385 train_time:1843ms step_avg:122.89ms
step:26/1385 train_time:1967ms step_avg:122.91ms
step:27/1385 train_time:2089ms step_avg:122.88ms
step:28/1385 train_time:2213ms step_avg:122.94ms
step:29/1385 train_time:2336ms step_avg:122.92ms
step:30/1385 train_time:2459ms step_avg:122.93ms
step:31/1385 train_time:2581ms step_avg:122.92ms
step:32/1385 train_time:2703ms step_avg:122.87ms
step:33/1385 train_time:2825ms step_avg:122.83ms
step:34/1385 train_time:2947ms step_avg:122.80ms
step:35/1385 train_time:3070ms step_avg:122.79ms
step:36/1385 train_time:3192ms step_avg:122.79ms
step:37/1385 train_time:3317ms step_avg:122.84ms
step:38/1385 train_time:3440ms step_avg:122.86ms
step:39/1385 train_time:3563ms step_avg:122.85ms
step:40/1385 train_time:3686ms step_avg:122.88ms
step:41/1385 train_time:3809ms step_avg:122.88ms
step:42/1385 train_time:3933ms step_avg:122.89ms
step:43/1385 train_time:4055ms step_avg:122.88ms
step:44/1385 train_time:4179ms step_avg:122.90ms
step:45/1385 train_time:4305ms step_avg:122.99ms
step:46/1385 train_time:4427ms step_avg:122.97ms
step:47/1385 train_time:4551ms step_avg:123.00ms
step:48/1385 train_time:4674ms step_avg:123.00ms
step:49/1385 train_time:4796ms step_avg:122.97ms
step:50/1385 train_time:4919ms step_avg:122.98ms
step:51/1385 train_time:5042ms step_avg:122.99ms
step:52/1385 train_time:5165ms step_avg:122.97ms
step:53/1385 train_time:5288ms step_avg:122.97ms
step:54/1385 train_time:5411ms step_avg:122.98ms
step:55/1385 train_time:5534ms step_avg:122.97ms
step:56/1385 train_time:5657ms step_avg:122.99ms
step:57/1385 train_time:5782ms step_avg:123.03ms
step:58/1385 train_time:5904ms step_avg:123.00ms
step:59/1385 train_time:6026ms step_avg:122.99ms
step:60/1385 train_time:6149ms step_avg:122.98ms
step:61/1385 train_time:6273ms step_avg:123.01ms
step:62/1385 train_time:6396ms step_avg:122.99ms
step:63/1385 train_time:6519ms step_avg:123.00ms
step:64/1385 train_time:6643ms step_avg:123.02ms
step:65/1385 train_time:6766ms step_avg:123.02ms
step:66/1385 train_time:6889ms step_avg:123.03ms
step:67/1385 train_time:7012ms step_avg:123.01ms
step:68/1385 train_time:7133ms step_avg:122.99ms
step:69/1385 train_time:7258ms step_avg:123.02ms
step:70/1385 train_time:7382ms step_avg:123.03ms
step:71/1385 train_time:7505ms step_avg:123.03ms
step:72/1385 train_time:7629ms step_avg:123.06ms
step:73/1385 train_time:7754ms step_avg:123.07ms
step:74/1385 train_time:7876ms step_avg:123.06ms
step:75/1385 train_time:7999ms step_avg:123.05ms
step:76/1385 train_time:8121ms step_avg:123.04ms
step:77/1385 train_time:8243ms step_avg:123.02ms
step:78/1385 train_time:8366ms step_avg:123.03ms
step:79/1385 train_time:8490ms step_avg:123.04ms
step:80/1385 train_time:8614ms step_avg:123.05ms
step:81/1385 train_time:8736ms step_avg:123.04ms
step:82/1385 train_time:8859ms step_avg:123.05ms
step:83/1385 train_time:8984ms step_avg:123.07ms
step:84/1385 train_time:9108ms step_avg:123.08ms
step:85/1385 train_time:9231ms step_avg:123.08ms
step:86/1385 train_time:9353ms step_avg:123.06ms
step:87/1385 train_time:9476ms step_avg:123.07ms
step:88/1385 train_time:9599ms step_avg:123.06ms
step:89/1385 train_time:9722ms step_avg:123.06ms
step:90/1385 train_time:9845ms step_avg:123.07ms
step:91/1385 train_time:9968ms step_avg:123.07ms
step:92/1385 train_time:10092ms step_avg:123.07ms
step:93/1385 train_time:10214ms step_avg:123.06ms
step:94/1385 train_time:10336ms step_avg:123.04ms
step:95/1385 train_time:10458ms step_avg:123.04ms
step:96/1385 train_time:10582ms step_avg:123.05ms
step:97/1385 train_time:10706ms step_avg:123.06ms
step:98/1385 train_time:10828ms step_avg:123.05ms
step:99/1385 train_time:10951ms step_avg:123.05ms
step:100/1385 train_time:11076ms step_avg:123.06ms
step:101/1385 train_time:11199ms step_avg:123.06ms
step:102/1385 train_time:11321ms step_avg:123.05ms
step:103/1385 train_time:11443ms step_avg:123.05ms
step:104/1385 train_time:11567ms step_avg:123.05ms
step:105/1385 train_time:11693ms step_avg:123.08ms
step:106/1385 train_time:11816ms step_avg:123.08ms
step:107/1385 train_time:11940ms step_avg:123.09ms
step:108/1385 train_time:12064ms step_avg:123.10ms
step:109/1385 train_time:12187ms step_avg:123.10ms
step:110/1385 train_time:12312ms step_avg:123.12ms
step:111/1385 train_time:12435ms step_avg:123.11ms
step:112/1385 train_time:12558ms step_avg:123.12ms
step:113/1385 train_time:12684ms step_avg:123.15ms
step:114/1385 train_time:12807ms step_avg:123.15ms
step:115/1385 train_time:12930ms step_avg:123.14ms
step:116/1385 train_time:13054ms step_avg:123.15ms
step:117/1385 train_time:13176ms step_avg:123.14ms
step:118/1385 train_time:13300ms step_avg:123.15ms
step:119/1385 train_time:13423ms step_avg:123.15ms
step:120/1385 train_time:13547ms step_avg:123.15ms
step:121/1385 train_time:13673ms step_avg:123.18ms
step:122/1385 train_time:13797ms step_avg:123.19ms
step:123/1385 train_time:13921ms step_avg:123.20ms
step:124/1385 train_time:14045ms step_avg:123.20ms
step:125/1385 train_time:14167ms step_avg:123.19ms
step:125/1385 val_loss:4.3930 train_time:14290ms step_avg:124.26ms
step:126/1385 train_time:14311ms step_avg:123.37ms
step:127/1385 train_time:14431ms step_avg:123.34ms
step:128/1385 train_time:14561ms step_avg:123.40ms
step:129/1385 train_time:14685ms step_avg:123.40ms
step:130/1385 train_time:14808ms step_avg:123.40ms
step:131/1385 train_time:14931ms step_avg:123.39ms
step:132/1385 train_time:15054ms step_avg:123.40ms
step:133/1385 train_time:15177ms step_avg:123.39ms
step:134/1385 train_time:15300ms step_avg:123.39ms
step:135/1385 train_time:15424ms step_avg:123.39ms
step:136/1385 train_time:15549ms step_avg:123.40ms
step:137/1385 train_time:15673ms step_avg:123.41ms
step:138/1385 train_time:15798ms step_avg:123.42ms
step:139/1385 train_time:15921ms step_avg:123.42ms
step:140/1385 train_time:16044ms step_avg:123.41ms
step:141/1385 train_time:16168ms step_avg:123.42ms
step:142/1385 train_time:16294ms step_avg:123.44ms
step:143/1385 train_time:16418ms step_avg:123.44ms
step:144/1385 train_time:16541ms step_avg:123.44ms
step:145/1385 train_time:16664ms step_avg:123.44ms
step:146/1385 train_time:16789ms step_avg:123.45ms
step:147/1385 train_time:16913ms step_avg:123.45ms
step:148/1385 train_time:17036ms step_avg:123.45ms
step:149/1385 train_time:17158ms step_avg:123.44ms
step:150/1385 train_time:17283ms step_avg:123.45ms
step:151/1385 train_time:17408ms step_avg:123.46ms
step:152/1385 train_time:17531ms step_avg:123.46ms
step:153/1385 train_time:17653ms step_avg:123.45ms
step:154/1385 train_time:17777ms step_avg:123.45ms
step:155/1385 train_time:17902ms step_avg:123.46ms
step:156/1385 train_time:18025ms step_avg:123.46ms
step:157/1385 train_time:18148ms step_avg:123.46ms
step:158/1385 train_time:18272ms step_avg:123.46ms
step:159/1385 train_time:18396ms step_avg:123.46ms
step:160/1385 train_time:18520ms step_avg:123.47ms
step:161/1385 train_time:18644ms step_avg:123.47ms
step:162/1385 train_time:18767ms step_avg:123.47ms
step:163/1385 train_time:18892ms step_avg:123.47ms
step:164/1385 train_time:19016ms step_avg:123.48ms
step:165/1385 train_time:19139ms step_avg:123.48ms
step:166/1385 train_time:19265ms step_avg:123.49ms
step:167/1385 train_time:19387ms step_avg:123.49ms
step:168/1385 train_time:19510ms step_avg:123.48ms
step:169/1385 train_time:19632ms step_avg:123.47ms
step:170/1385 train_time:19757ms step_avg:123.48ms
step:171/1385 train_time:19880ms step_avg:123.48ms
step:172/1385 train_time:20004ms step_avg:123.48ms
step:173/1385 train_time:20129ms step_avg:123.49ms
step:174/1385 train_time:20253ms step_avg:123.50ms
step:175/1385 train_time:20379ms step_avg:123.51ms
step:176/1385 train_time:20502ms step_avg:123.51ms
step:177/1385 train_time:20626ms step_avg:123.51ms
step:178/1385 train_time:20751ms step_avg:123.52ms
step:179/1385 train_time:20875ms step_avg:123.52ms
step:180/1385 train_time:20998ms step_avg:123.52ms
step:181/1385 train_time:21121ms step_avg:123.51ms
step:182/1385 train_time:21245ms step_avg:123.52ms
step:183/1385 train_time:21368ms step_avg:123.52ms
step:184/1385 train_time:21491ms step_avg:123.51ms
step:185/1385 train_time:21616ms step_avg:123.52ms
step:186/1385 train_time:21739ms step_avg:123.52ms
step:187/1385 train_time:21862ms step_avg:123.51ms
step:188/1385 train_time:21986ms step_avg:123.52ms
step:189/1385 train_time:22110ms step_avg:123.52ms
step:190/1385 train_time:22233ms step_avg:123.52ms
step:191/1385 train_time:22358ms step_avg:123.52ms
step:192/1385 train_time:22481ms step_avg:123.52ms
step:193/1385 train_time:22604ms step_avg:123.52ms
step:194/1385 train_time:22728ms step_avg:123.52ms
step:195/1385 train_time:22851ms step_avg:123.52ms
step:196/1385 train_time:22975ms step_avg:123.52ms
step:197/1385 train_time:23098ms step_avg:123.52ms
step:198/1385 train_time:23222ms step_avg:123.52ms
step:199/1385 train_time:23345ms step_avg:123.52ms
step:200/1385 train_time:23470ms step_avg:123.53ms
step:201/1385 train_time:23594ms step_avg:123.53ms
step:202/1385 train_time:23719ms step_avg:123.53ms
step:203/1385 train_time:23842ms step_avg:123.53ms
step:204/1385 train_time:23966ms step_avg:123.53ms
step:205/1385 train_time:24090ms step_avg:123.54ms
step:206/1385 train_time:24214ms step_avg:123.54ms
step:207/1385 train_time:24338ms step_avg:123.54ms
step:208/1385 train_time:24462ms step_avg:123.54ms
step:209/1385 train_time:24586ms step_avg:123.55ms
step:210/1385 train_time:24710ms step_avg:123.55ms
step:211/1385 train_time:24836ms step_avg:123.56ms
step:212/1385 train_time:24959ms step_avg:123.56ms
step:213/1385 train_time:25082ms step_avg:123.56ms
step:214/1385 train_time:25206ms step_avg:123.56ms
step:215/1385 train_time:25330ms step_avg:123.56ms
step:216/1385 train_time:25454ms step_avg:123.56ms
step:217/1385 train_time:25577ms step_avg:123.56ms
step:218/1385 train_time:25701ms step_avg:123.56ms
step:219/1385 train_time:25825ms step_avg:123.56ms
step:220/1385 train_time:25948ms step_avg:123.56ms
step:221/1385 train_time:26071ms step_avg:123.56ms
step:222/1385 train_time:26196ms step_avg:123.57ms
step:223/1385 train_time:26319ms step_avg:123.57ms
step:224/1385 train_time:26443ms step_avg:123.57ms
step:225/1385 train_time:26566ms step_avg:123.56ms
step:226/1385 train_time:26691ms step_avg:123.57ms
step:227/1385 train_time:26813ms step_avg:123.56ms
step:228/1385 train_time:26938ms step_avg:123.57ms
step:229/1385 train_time:27062ms step_avg:123.57ms
step:230/1385 train_time:27185ms step_avg:123.57ms
step:231/1385 train_time:27309ms step_avg:123.57ms
step:232/1385 train_time:27433ms step_avg:123.57ms
step:233/1385 train_time:27557ms step_avg:123.57ms
step:234/1385 train_time:27681ms step_avg:123.58ms
step:235/1385 train_time:27805ms step_avg:123.58ms
step:236/1385 train_time:27930ms step_avg:123.58ms
step:237/1385 train_time:28054ms step_avg:123.59ms
step:238/1385 train_time:28178ms step_avg:123.59ms
step:239/1385 train_time:28302ms step_avg:123.59ms
step:240/1385 train_time:28426ms step_avg:123.59ms
step:241/1385 train_time:28551ms step_avg:123.60ms
step:242/1385 train_time:28674ms step_avg:123.60ms
step:243/1385 train_time:28799ms step_avg:123.60ms
step:244/1385 train_time:28923ms step_avg:123.60ms
step:245/1385 train_time:29048ms step_avg:123.61ms
step:246/1385 train_time:29172ms step_avg:123.61ms
step:247/1385 train_time:29296ms step_avg:123.61ms
step:248/1385 train_time:29420ms step_avg:123.62ms
step:249/1385 train_time:29545ms step_avg:123.62ms
step:250/1385 train_time:29670ms step_avg:123.63ms
step:250/1385 val_loss:3.9752 train_time:29792ms step_avg:124.13ms
step:251/1385 train_time:29813ms step_avg:123.70ms
step:252/1385 train_time:29932ms step_avg:123.68ms
step:253/1385 train_time:30061ms step_avg:123.71ms
step:254/1385 train_time:30185ms step_avg:123.71ms
step:255/1385 train_time:30308ms step_avg:123.71ms
step:256/1385 train_time:30432ms step_avg:123.71ms
step:257/1385 train_time:30555ms step_avg:123.70ms
step:258/1385 train_time:30678ms step_avg:123.70ms
step:259/1385 train_time:30801ms step_avg:123.70ms
step:260/1385 train_time:30926ms step_avg:123.70ms
step:261/1385 train_time:31050ms step_avg:123.71ms
step:262/1385 train_time:31175ms step_avg:123.71ms
step:263/1385 train_time:31299ms step_avg:123.71ms
step:264/1385 train_time:31422ms step_avg:123.71ms
step:265/1385 train_time:31547ms step_avg:123.71ms
step:266/1385 train_time:31669ms step_avg:123.71ms
step:267/1385 train_time:31793ms step_avg:123.71ms
step:268/1385 train_time:31916ms step_avg:123.71ms
step:269/1385 train_time:32040ms step_avg:123.71ms
step:270/1385 train_time:32166ms step_avg:123.71ms
step:271/1385 train_time:32290ms step_avg:123.71ms
step:272/1385 train_time:32414ms step_avg:123.72ms
step:273/1385 train_time:32538ms step_avg:123.72ms
step:274/1385 train_time:32662ms step_avg:123.72ms
step:275/1385 train_time:32785ms step_avg:123.72ms
step:276/1385 train_time:32908ms step_avg:123.72ms
step:277/1385 train_time:33032ms step_avg:123.72ms
step:278/1385 train_time:33158ms step_avg:123.72ms
step:279/1385 train_time:33282ms step_avg:123.73ms
step:280/1385 train_time:33406ms step_avg:123.73ms
step:281/1385 train_time:33530ms step_avg:123.73ms
step:282/1385 train_time:33654ms step_avg:123.73ms
step:283/1385 train_time:33778ms step_avg:123.73ms
step:284/1385 train_time:33902ms step_avg:123.73ms
step:285/1385 train_time:34026ms step_avg:123.73ms
step:286/1385 train_time:34149ms step_avg:123.73ms
step:287/1385 train_time:34274ms step_avg:123.73ms
step:288/1385 train_time:34396ms step_avg:123.73ms
step:289/1385 train_time:34521ms step_avg:123.73ms
step:290/1385 train_time:34645ms step_avg:123.73ms
step:291/1385 train_time:34769ms step_avg:123.73ms
step:292/1385 train_time:34892ms step_avg:123.73ms
step:293/1385 train_time:35015ms step_avg:123.73ms
step:294/1385 train_time:35139ms step_avg:123.73ms
step:295/1385 train_time:35264ms step_avg:123.73ms
step:296/1385 train_time:35388ms step_avg:123.73ms
step:297/1385 train_time:35513ms step_avg:123.74ms
step:298/1385 train_time:35636ms step_avg:123.74ms
step:299/1385 train_time:35760ms step_avg:123.74ms
step:300/1385 train_time:35884ms step_avg:123.74ms
step:301/1385 train_time:36008ms step_avg:123.74ms
step:302/1385 train_time:36131ms step_avg:123.74ms
step:303/1385 train_time:36256ms step_avg:123.74ms
step:304/1385 train_time:36379ms step_avg:123.74ms
step:305/1385 train_time:36503ms step_avg:123.74ms
step:306/1385 train_time:36627ms step_avg:123.74ms
step:307/1385 train_time:36751ms step_avg:123.74ms
step:308/1385 train_time:36875ms step_avg:123.74ms
step:309/1385 train_time:36998ms step_avg:123.74ms
step:310/1385 train_time:37124ms step_avg:123.75ms
step:311/1385 train_time:37250ms step_avg:123.75ms
step:312/1385 train_time:37377ms step_avg:123.77ms
step:313/1385 train_time:37505ms step_avg:123.78ms
step:314/1385 train_time:37631ms step_avg:123.79ms
step:315/1385 train_time:37758ms step_avg:123.80ms
step:316/1385 train_time:37883ms step_avg:123.80ms
step:317/1385 train_time:38010ms step_avg:123.81ms
step:318/1385 train_time:38136ms step_avg:123.82ms
step:319/1385 train_time:38263ms step_avg:123.83ms
step:320/1385 train_time:38389ms step_avg:123.83ms
step:321/1385 train_time:38517ms step_avg:123.85ms
step:322/1385 train_time:38642ms step_avg:123.85ms
step:323/1385 train_time:38769ms step_avg:123.86ms
step:324/1385 train_time:38895ms step_avg:123.87ms
step:325/1385 train_time:39022ms step_avg:123.88ms
step:326/1385 train_time:39148ms step_avg:123.89ms
step:327/1385 train_time:39275ms step_avg:123.89ms
step:328/1385 train_time:39401ms step_avg:123.90ms
step:329/1385 train_time:39528ms step_avg:123.91ms
step:330/1385 train_time:39655ms step_avg:123.92ms
step:331/1385 train_time:39781ms step_avg:123.93ms
step:332/1385 train_time:39908ms step_avg:123.94ms
step:333/1385 train_time:40034ms step_avg:123.95ms
step:334/1385 train_time:40160ms step_avg:123.95ms
step:335/1385 train_time:40287ms step_avg:123.96ms
step:336/1385 train_time:40413ms step_avg:123.97ms
step:337/1385 train_time:40541ms step_avg:123.98ms
step:338/1385 train_time:40667ms step_avg:123.98ms
step:339/1385 train_time:40794ms step_avg:123.99ms
step:340/1385 train_time:40919ms step_avg:124.00ms
step:341/1385 train_time:41045ms step_avg:124.00ms
step:342/1385 train_time:41171ms step_avg:124.01ms
step:343/1385 train_time:41298ms step_avg:124.02ms
step:344/1385 train_time:41423ms step_avg:124.02ms
step:345/1385 train_time:41549ms step_avg:124.03ms
step:346/1385 train_time:41676ms step_avg:124.04ms
step:347/1385 train_time:41804ms step_avg:124.05ms
step:348/1385 train_time:41930ms step_avg:124.05ms
step:349/1385 train_time:42057ms step_avg:124.06ms
step:350/1385 train_time:42184ms step_avg:124.07ms
step:351/1385 train_time:42309ms step_avg:124.07ms
step:352/1385 train_time:42436ms step_avg:124.08ms
step:353/1385 train_time:42562ms step_avg:124.09ms
step:354/1385 train_time:42688ms step_avg:124.09ms
step:355/1385 train_time:42815ms step_avg:124.10ms
step:356/1385 train_time:42941ms step_avg:124.11ms
step:357/1385 train_time:43067ms step_avg:124.11ms
step:358/1385 train_time:43192ms step_avg:124.12ms
step:359/1385 train_time:43319ms step_avg:124.12ms
step:360/1385 train_time:43445ms step_avg:124.13ms
step:361/1385 train_time:43571ms step_avg:124.13ms
step:362/1385 train_time:43697ms step_avg:124.14ms
step:363/1385 train_time:43823ms step_avg:124.14ms
step:364/1385 train_time:43949ms step_avg:124.15ms
step:365/1385 train_time:44076ms step_avg:124.16ms
step:366/1385 train_time:44203ms step_avg:124.17ms
step:367/1385 train_time:44329ms step_avg:124.17ms
step:368/1385 train_time:44456ms step_avg:124.18ms
step:369/1385 train_time:44582ms step_avg:124.18ms
step:370/1385 train_time:44708ms step_avg:124.19ms
step:371/1385 train_time:44834ms step_avg:124.19ms
step:372/1385 train_time:44960ms step_avg:124.20ms
step:373/1385 train_time:45087ms step_avg:124.21ms
step:374/1385 train_time:45214ms step_avg:124.21ms
step:375/1385 train_time:45341ms step_avg:124.22ms
step:375/1385 val_loss:3.7789 train_time:45466ms step_avg:124.56ms
step:376/1385 train_time:45487ms step_avg:124.28ms
step:377/1385 train_time:45608ms step_avg:124.27ms
step:378/1385 train_time:45736ms step_avg:124.28ms
step:379/1385 train_time:45862ms step_avg:124.29ms
step:380/1385 train_time:45987ms step_avg:124.29ms
step:381/1385 train_time:46113ms step_avg:124.29ms
step:382/1385 train_time:46238ms step_avg:124.30ms
step:383/1385 train_time:46364ms step_avg:124.30ms
step:384/1385 train_time:46492ms step_avg:124.31ms
step:385/1385 train_time:46620ms step_avg:124.32ms
step:386/1385 train_time:46747ms step_avg:124.33ms
step:387/1385 train_time:46874ms step_avg:124.33ms
step:388/1385 train_time:46999ms step_avg:124.34ms
step:389/1385 train_time:47126ms step_avg:124.34ms
step:390/1385 train_time:47252ms step_avg:124.35ms
step:391/1385 train_time:47377ms step_avg:124.35ms
step:392/1385 train_time:47503ms step_avg:124.35ms
step:393/1385 train_time:47630ms step_avg:124.36ms
step:394/1385 train_time:47757ms step_avg:124.37ms
step:395/1385 train_time:47884ms step_avg:124.37ms
step:396/1385 train_time:48010ms step_avg:124.38ms
step:397/1385 train_time:48136ms step_avg:124.38ms
step:398/1385 train_time:48262ms step_avg:124.39ms
step:399/1385 train_time:48388ms step_avg:124.39ms
step:400/1385 train_time:48514ms step_avg:124.40ms
step:401/1385 train_time:48640ms step_avg:124.40ms
step:402/1385 train_time:48767ms step_avg:124.40ms
step:403/1385 train_time:48895ms step_avg:124.41ms
step:404/1385 train_time:49021ms step_avg:124.42ms
step:405/1385 train_time:49148ms step_avg:124.42ms
step:406/1385 train_time:49274ms step_avg:124.43ms
step:407/1385 train_time:49399ms step_avg:124.43ms
step:408/1385 train_time:49526ms step_avg:124.44ms
step:409/1385 train_time:49652ms step_avg:124.44ms
step:410/1385 train_time:49778ms step_avg:124.45ms
step:411/1385 train_time:49905ms step_avg:124.45ms
step:412/1385 train_time:50032ms step_avg:124.46ms
step:413/1385 train_time:50158ms step_avg:124.46ms
step:414/1385 train_time:50285ms step_avg:124.47ms
step:415/1385 train_time:50412ms step_avg:124.47ms
step:416/1385 train_time:50538ms step_avg:124.48ms
step:417/1385 train_time:50665ms step_avg:124.48ms
step:418/1385 train_time:50792ms step_avg:124.49ms
step:419/1385 train_time:50920ms step_avg:124.50ms
step:420/1385 train_time:51047ms step_avg:124.50ms
step:421/1385 train_time:51174ms step_avg:124.51ms
step:422/1385 train_time:51301ms step_avg:124.52ms
step:423/1385 train_time:51428ms step_avg:124.52ms
step:424/1385 train_time:51556ms step_avg:124.53ms
step:425/1385 train_time:51683ms step_avg:124.54ms
step:426/1385 train_time:51809ms step_avg:124.54ms
step:427/1385 train_time:51937ms step_avg:124.55ms
step:428/1385 train_time:52064ms step_avg:124.56ms
step:429/1385 train_time:52192ms step_avg:124.56ms
step:430/1385 train_time:52318ms step_avg:124.57ms
step:431/1385 train_time:52445ms step_avg:124.57ms
step:432/1385 train_time:52573ms step_avg:124.58ms
step:433/1385 train_time:52699ms step_avg:124.58ms
step:434/1385 train_time:52827ms step_avg:124.59ms
step:435/1385 train_time:52954ms step_avg:124.60ms
step:436/1385 train_time:53080ms step_avg:124.60ms
step:437/1385 train_time:53208ms step_avg:124.61ms
step:438/1385 train_time:53336ms step_avg:124.62ms
step:439/1385 train_time:53462ms step_avg:124.62ms
step:440/1385 train_time:53589ms step_avg:124.62ms
step:441/1385 train_time:53715ms step_avg:124.63ms
step:442/1385 train_time:53841ms step_avg:124.63ms
step:443/1385 train_time:53967ms step_avg:124.64ms
step:444/1385 train_time:54094ms step_avg:124.64ms
step:445/1385 train_time:54220ms step_avg:124.64ms
step:446/1385 train_time:54347ms step_avg:124.65ms
step:447/1385 train_time:54474ms step_avg:124.66ms
step:448/1385 train_time:54602ms step_avg:124.66ms
step:449/1385 train_time:54728ms step_avg:124.67ms
step:450/1385 train_time:54855ms step_avg:124.67ms
step:451/1385 train_time:54982ms step_avg:124.68ms
step:452/1385 train_time:55108ms step_avg:124.68ms
step:453/1385 train_time:55235ms step_avg:124.68ms
step:454/1385 train_time:55363ms step_avg:124.69ms
step:455/1385 train_time:55489ms step_avg:124.70ms
step:456/1385 train_time:55617ms step_avg:124.70ms
step:457/1385 train_time:55744ms step_avg:124.71ms
step:458/1385 train_time:55871ms step_avg:124.71ms
step:459/1385 train_time:55998ms step_avg:124.72ms
step:460/1385 train_time:56125ms step_avg:124.72ms
step:461/1385 train_time:56251ms step_avg:124.72ms
step:462/1385 train_time:56378ms step_avg:124.73ms
step:463/1385 train_time:56505ms step_avg:124.74ms
step:464/1385 train_time:56633ms step_avg:124.74ms
step:465/1385 train_time:56760ms step_avg:124.75ms
step:466/1385 train_time:56887ms step_avg:124.75ms
step:467/1385 train_time:57014ms step_avg:124.76ms
step:468/1385 train_time:57140ms step_avg:124.76ms
step:469/1385 train_time:57268ms step_avg:124.77ms
step:470/1385 train_time:57394ms step_avg:124.77ms
step:471/1385 train_time:57520ms step_avg:124.77ms
step:472/1385 train_time:57647ms step_avg:124.78ms
step:473/1385 train_time:57774ms step_avg:124.78ms
step:474/1385 train_time:57902ms step_avg:124.79ms
step:475/1385 train_time:58028ms step_avg:124.79ms
step:476/1385 train_time:58154ms step_avg:124.79ms
step:477/1385 train_time:58282ms step_avg:124.80ms
step:478/1385 train_time:58409ms step_avg:124.81ms
step:479/1385 train_time:58537ms step_avg:124.81ms
step:480/1385 train_time:58665ms step_avg:124.82ms
step:481/1385 train_time:58791ms step_avg:124.82ms
step:482/1385 train_time:58919ms step_avg:124.83ms
step:483/1385 train_time:59046ms step_avg:124.83ms
step:484/1385 train_time:59173ms step_avg:124.84ms
step:485/1385 train_time:59301ms step_avg:124.84ms
step:486/1385 train_time:59428ms step_avg:124.85ms
step:487/1385 train_time:59555ms step_avg:124.85ms
step:488/1385 train_time:59681ms step_avg:124.86ms
step:489/1385 train_time:59807ms step_avg:124.86ms
step:490/1385 train_time:59934ms step_avg:124.86ms
step:491/1385 train_time:60061ms step_avg:124.87ms
step:492/1385 train_time:60188ms step_avg:124.87ms
step:493/1385 train_time:60315ms step_avg:124.87ms
step:494/1385 train_time:60441ms step_avg:124.88ms
step:495/1385 train_time:60568ms step_avg:124.88ms
step:496/1385 train_time:60696ms step_avg:124.89ms
step:497/1385 train_time:60822ms step_avg:124.89ms
step:498/1385 train_time:60950ms step_avg:124.90ms
step:499/1385 train_time:61077ms step_avg:124.90ms
step:500/1385 train_time:61203ms step_avg:124.90ms
step:500/1385 val_loss:3.6612 train_time:61329ms step_avg:125.16ms
step:501/1385 train_time:61349ms step_avg:124.95ms
step:502/1385 train_time:61472ms step_avg:124.94ms
step:503/1385 train_time:61602ms step_avg:124.95ms
step:504/1385 train_time:61728ms step_avg:124.96ms
step:505/1385 train_time:61854ms step_avg:124.96ms
step:506/1385 train_time:61980ms step_avg:124.96ms
step:507/1385 train_time:62107ms step_avg:124.96ms
step:508/1385 train_time:62233ms step_avg:124.97ms
step:509/1385 train_time:62360ms step_avg:124.97ms
step:510/1385 train_time:62488ms step_avg:124.98ms
step:511/1385 train_time:62617ms step_avg:124.98ms
step:512/1385 train_time:62743ms step_avg:124.99ms
step:513/1385 train_time:62870ms step_avg:124.99ms
step:514/1385 train_time:62996ms step_avg:124.99ms
step:515/1385 train_time:63125ms step_avg:125.00ms
step:516/1385 train_time:63253ms step_avg:125.01ms
step:517/1385 train_time:63383ms step_avg:125.02ms
step:518/1385 train_time:63513ms step_avg:125.02ms
step:519/1385 train_time:63643ms step_avg:125.04ms
step:520/1385 train_time:63773ms step_avg:125.04ms
step:521/1385 train_time:63901ms step_avg:125.05ms
step:522/1385 train_time:64030ms step_avg:125.06ms
step:523/1385 train_time:64159ms step_avg:125.07ms
step:524/1385 train_time:64287ms step_avg:125.07ms
step:525/1385 train_time:64417ms step_avg:125.08ms
step:526/1385 train_time:64546ms step_avg:125.09ms
step:527/1385 train_time:64676ms step_avg:125.10ms
step:528/1385 train_time:64805ms step_avg:125.11ms
step:529/1385 train_time:64935ms step_avg:125.12ms
step:530/1385 train_time:65064ms step_avg:125.12ms
step:531/1385 train_time:65193ms step_avg:125.13ms
step:532/1385 train_time:65322ms step_avg:125.14ms
step:533/1385 train_time:65451ms step_avg:125.15ms
step:534/1385 train_time:65580ms step_avg:125.15ms
step:535/1385 train_time:65709ms step_avg:125.16ms
step:536/1385 train_time:65839ms step_avg:125.17ms
step:537/1385 train_time:65968ms step_avg:125.18ms
step:538/1385 train_time:66097ms step_avg:125.18ms
step:539/1385 train_time:66226ms step_avg:125.19ms
step:540/1385 train_time:66355ms step_avg:125.20ms
step:541/1385 train_time:66484ms step_avg:125.21ms
step:542/1385 train_time:66615ms step_avg:125.22ms
step:543/1385 train_time:66743ms step_avg:125.22ms
step:544/1385 train_time:66872ms step_avg:125.23ms
step:545/1385 train_time:67001ms step_avg:125.23ms
step:546/1385 train_time:67130ms step_avg:125.24ms
step:547/1385 train_time:67260ms step_avg:125.25ms
step:548/1385 train_time:67388ms step_avg:125.26ms
step:549/1385 train_time:67518ms step_avg:125.26ms
step:550/1385 train_time:67646ms step_avg:125.27ms
step:551/1385 train_time:67775ms step_avg:125.28ms
step:552/1385 train_time:67905ms step_avg:125.29ms
step:553/1385 train_time:68034ms step_avg:125.29ms
step:554/1385 train_time:68163ms step_avg:125.30ms
step:555/1385 train_time:68293ms step_avg:125.31ms
step:556/1385 train_time:68422ms step_avg:125.31ms
step:557/1385 train_time:68550ms step_avg:125.32ms
step:558/1385 train_time:68679ms step_avg:125.33ms
step:559/1385 train_time:68807ms step_avg:125.33ms
step:560/1385 train_time:68937ms step_avg:125.34ms
step:561/1385 train_time:69066ms step_avg:125.35ms
step:562/1385 train_time:69195ms step_avg:125.35ms
step:563/1385 train_time:69325ms step_avg:125.36ms
step:564/1385 train_time:69455ms step_avg:125.37ms
step:565/1385 train_time:69584ms step_avg:125.38ms
step:566/1385 train_time:69712ms step_avg:125.38ms
step:567/1385 train_time:69841ms step_avg:125.39ms
step:568/1385 train_time:69969ms step_avg:125.39ms
step:569/1385 train_time:70098ms step_avg:125.40ms
step:570/1385 train_time:70226ms step_avg:125.40ms
step:571/1385 train_time:70356ms step_avg:125.41ms
step:572/1385 train_time:70485ms step_avg:125.42ms
step:573/1385 train_time:70615ms step_avg:125.43ms
step:574/1385 train_time:70744ms step_avg:125.43ms
step:575/1385 train_time:70873ms step_avg:125.44ms
step:576/1385 train_time:71003ms step_avg:125.45ms
step:577/1385 train_time:71131ms step_avg:125.45ms
step:578/1385 train_time:71259ms step_avg:125.46ms
step:579/1385 train_time:71388ms step_avg:125.46ms
step:580/1385 train_time:71518ms step_avg:125.47ms
step:581/1385 train_time:71648ms step_avg:125.48ms
step:582/1385 train_time:71777ms step_avg:125.48ms
step:583/1385 train_time:71905ms step_avg:125.49ms
step:584/1385 train_time:72034ms step_avg:125.49ms
step:585/1385 train_time:72162ms step_avg:125.50ms
step:586/1385 train_time:72291ms step_avg:125.51ms
step:587/1385 train_time:72420ms step_avg:125.51ms
step:588/1385 train_time:72547ms step_avg:125.51ms
step:589/1385 train_time:72677ms step_avg:125.52ms
step:590/1385 train_time:72806ms step_avg:125.53ms
step:591/1385 train_time:72935ms step_avg:125.53ms
step:592/1385 train_time:73065ms step_avg:125.54ms
step:593/1385 train_time:73194ms step_avg:125.55ms
step:594/1385 train_time:73324ms step_avg:125.55ms
step:595/1385 train_time:73453ms step_avg:125.56ms
step:596/1385 train_time:73583ms step_avg:125.57ms
step:597/1385 train_time:73712ms step_avg:125.57ms
step:598/1385 train_time:73842ms step_avg:125.58ms
step:599/1385 train_time:73971ms step_avg:125.59ms
step:600/1385 train_time:74100ms step_avg:125.59ms
step:601/1385 train_time:74230ms step_avg:125.60ms
step:602/1385 train_time:74359ms step_avg:125.61ms
step:603/1385 train_time:74487ms step_avg:125.61ms
step:604/1385 train_time:74616ms step_avg:125.62ms
step:605/1385 train_time:74745ms step_avg:125.62ms
step:606/1385 train_time:74875ms step_avg:125.63ms
step:607/1385 train_time:75004ms step_avg:125.64ms
step:608/1385 train_time:75132ms step_avg:125.64ms
step:609/1385 train_time:75263ms step_avg:125.65ms
step:610/1385 train_time:75392ms step_avg:125.65ms
step:611/1385 train_time:75520ms step_avg:125.66ms
step:612/1385 train_time:75649ms step_avg:125.66ms
step:613/1385 train_time:75778ms step_avg:125.67ms
step:614/1385 train_time:75906ms step_avg:125.67ms
step:615/1385 train_time:76036ms step_avg:125.68ms
step:616/1385 train_time:76166ms step_avg:125.69ms
step:617/1385 train_time:76296ms step_avg:125.69ms
step:618/1385 train_time:76425ms step_avg:125.70ms
step:619/1385 train_time:76554ms step_avg:125.70ms
step:620/1385 train_time:76685ms step_avg:125.71ms
step:621/1385 train_time:76814ms step_avg:125.72ms
step:622/1385 train_time:76944ms step_avg:125.73ms
step:623/1385 train_time:77073ms step_avg:125.73ms
step:624/1385 train_time:77202ms step_avg:125.74ms
step:625/1385 train_time:77332ms step_avg:125.74ms
step:625/1385 val_loss:3.5777 train_time:77461ms step_avg:125.95ms
step:626/1385 train_time:77481ms step_avg:125.78ms
step:627/1385 train_time:77606ms step_avg:125.78ms
step:628/1385 train_time:77736ms step_avg:125.79ms
step:629/1385 train_time:77865ms step_avg:125.79ms
step:630/1385 train_time:77994ms step_avg:125.80ms
step:631/1385 train_time:78122ms step_avg:125.80ms
step:632/1385 train_time:78252ms step_avg:125.81ms
step:633/1385 train_time:78380ms step_avg:125.81ms
step:634/1385 train_time:78509ms step_avg:125.82ms
step:635/1385 train_time:78639ms step_avg:125.82ms
step:636/1385 train_time:78768ms step_avg:125.83ms
step:637/1385 train_time:78897ms step_avg:125.83ms
step:638/1385 train_time:79027ms step_avg:125.84ms
step:639/1385 train_time:79156ms step_avg:125.84ms
step:640/1385 train_time:79286ms step_avg:125.85ms
step:641/1385 train_time:79415ms step_avg:125.86ms
step:642/1385 train_time:79544ms step_avg:125.86ms
step:643/1385 train_time:79673ms step_avg:125.87ms
step:644/1385 train_time:79803ms step_avg:125.87ms
step:645/1385 train_time:79932ms step_avg:125.88ms
step:646/1385 train_time:80063ms step_avg:125.88ms
step:647/1385 train_time:80191ms step_avg:125.89ms
step:648/1385 train_time:80320ms step_avg:125.89ms
step:649/1385 train_time:80449ms step_avg:125.90ms
step:650/1385 train_time:80579ms step_avg:125.90ms
step:651/1385 train_time:80710ms step_avg:125.91ms
step:652/1385 train_time:80838ms step_avg:125.92ms
step:653/1385 train_time:80968ms step_avg:125.92ms
step:654/1385 train_time:81098ms step_avg:125.93ms
step:655/1385 train_time:81227ms step_avg:125.93ms
step:656/1385 train_time:81356ms step_avg:125.94ms
step:657/1385 train_time:81486ms step_avg:125.94ms
step:658/1385 train_time:81615ms step_avg:125.95ms
step:659/1385 train_time:81745ms step_avg:125.96ms
step:660/1385 train_time:81874ms step_avg:125.96ms
step:661/1385 train_time:82004ms step_avg:125.97ms
step:662/1385 train_time:82133ms step_avg:125.97ms
step:663/1385 train_time:82262ms step_avg:125.98ms
step:664/1385 train_time:82392ms step_avg:125.98ms
step:665/1385 train_time:82521ms step_avg:125.99ms
step:666/1385 train_time:82650ms step_avg:125.99ms
step:667/1385 train_time:82780ms step_avg:126.00ms
step:668/1385 train_time:82909ms step_avg:126.00ms
step:669/1385 train_time:83039ms step_avg:126.01ms
step:670/1385 train_time:83168ms step_avg:126.01ms
step:671/1385 train_time:83298ms step_avg:126.02ms
step:672/1385 train_time:83427ms step_avg:126.02ms
step:673/1385 train_time:83557ms step_avg:126.03ms
step:674/1385 train_time:83687ms step_avg:126.03ms
step:675/1385 train_time:83817ms step_avg:126.04ms
step:676/1385 train_time:83948ms step_avg:126.05ms
step:677/1385 train_time:84078ms step_avg:126.05ms
step:678/1385 train_time:84207ms step_avg:126.06ms
step:679/1385 train_time:84338ms step_avg:126.07ms
step:680/1385 train_time:84467ms step_avg:126.07ms
step:681/1385 train_time:84596ms step_avg:126.07ms
step:682/1385 train_time:84726ms step_avg:126.08ms
step:683/1385 train_time:84855ms step_avg:126.08ms
step:684/1385 train_time:84985ms step_avg:126.09ms
step:685/1385 train_time:85114ms step_avg:126.09ms
step:686/1385 train_time:85244ms step_avg:126.10ms
step:687/1385 train_time:85373ms step_avg:126.11ms
step:688/1385 train_time:85503ms step_avg:126.11ms
step:689/1385 train_time:85632ms step_avg:126.12ms
step:690/1385 train_time:85761ms step_avg:126.12ms
step:691/1385 train_time:85892ms step_avg:126.13ms
step:692/1385 train_time:86021ms step_avg:126.13ms
step:693/1385 train_time:86152ms step_avg:126.14ms
step:694/1385 train_time:86281ms step_avg:126.14ms
step:695/1385 train_time:86409ms step_avg:126.14ms
step:696/1385 train_time:86538ms step_avg:126.15ms
step:697/1385 train_time:86666ms step_avg:126.15ms
step:698/1385 train_time:86796ms step_avg:126.16ms
step:699/1385 train_time:86926ms step_avg:126.16ms
step:700/1385 train_time:87055ms step_avg:126.17ms
step:701/1385 train_time:87185ms step_avg:126.17ms
step:702/1385 train_time:87315ms step_avg:126.18ms
step:703/1385 train_time:87444ms step_avg:126.18ms
step:704/1385 train_time:87573ms step_avg:126.19ms
step:705/1385 train_time:87702ms step_avg:126.19ms
step:706/1385 train_time:87832ms step_avg:126.20ms
step:707/1385 train_time:87961ms step_avg:126.20ms
step:708/1385 train_time:88090ms step_avg:126.20ms
step:709/1385 train_time:88221ms step_avg:126.21ms
step:710/1385 train_time:88350ms step_avg:126.21ms
step:711/1385 train_time:88479ms step_avg:126.22ms
step:712/1385 train_time:88608ms step_avg:126.22ms
step:713/1385 train_time:88737ms step_avg:126.23ms
step:714/1385 train_time:88867ms step_avg:126.23ms
step:715/1385 train_time:88996ms step_avg:126.24ms
step:716/1385 train_time:89126ms step_avg:126.24ms
step:717/1385 train_time:89255ms step_avg:126.24ms
step:718/1385 train_time:89386ms step_avg:126.25ms
step:719/1385 train_time:89515ms step_avg:126.25ms
step:720/1385 train_time:89645ms step_avg:126.26ms
step:721/1385 train_time:89776ms step_avg:126.27ms
step:722/1385 train_time:89908ms step_avg:126.27ms
step:723/1385 train_time:90039ms step_avg:126.28ms
step:724/1385 train_time:90170ms step_avg:126.29ms
step:725/1385 train_time:90302ms step_avg:126.30ms
step:726/1385 train_time:90434ms step_avg:126.30ms
step:727/1385 train_time:90566ms step_avg:126.31ms
step:728/1385 train_time:90697ms step_avg:126.32ms
step:729/1385 train_time:90828ms step_avg:126.33ms
step:730/1385 train_time:90960ms step_avg:126.33ms
step:731/1385 train_time:91092ms step_avg:126.34ms
step:732/1385 train_time:91222ms step_avg:126.35ms
step:733/1385 train_time:91355ms step_avg:126.36ms
step:734/1385 train_time:91486ms step_avg:126.36ms
step:735/1385 train_time:91618ms step_avg:126.37ms
step:736/1385 train_time:91750ms step_avg:126.38ms
step:737/1385 train_time:91881ms step_avg:126.38ms
step:738/1385 train_time:92012ms step_avg:126.39ms
step:739/1385 train_time:92143ms step_avg:126.40ms
step:740/1385 train_time:92275ms step_avg:126.40ms
step:741/1385 train_time:92408ms step_avg:126.41ms
step:742/1385 train_time:92539ms step_avg:126.42ms
step:743/1385 train_time:92669ms step_avg:126.42ms
step:744/1385 train_time:92802ms step_avg:126.43ms
step:745/1385 train_time:92933ms step_avg:126.44ms
step:746/1385 train_time:93064ms step_avg:126.45ms
step:747/1385 train_time:93195ms step_avg:126.45ms
step:748/1385 train_time:93327ms step_avg:126.46ms
step:749/1385 train_time:93460ms step_avg:126.47ms
step:750/1385 train_time:93592ms step_avg:126.48ms
step:750/1385 val_loss:3.5265 train_time:93723ms step_avg:126.65ms
step:751/1385 train_time:93743ms step_avg:126.51ms
step:752/1385 train_time:93867ms step_avg:126.51ms
step:753/1385 train_time:94000ms step_avg:126.51ms
step:754/1385 train_time:94130ms step_avg:126.52ms
step:755/1385 train_time:94262ms step_avg:126.53ms
step:756/1385 train_time:94392ms step_avg:126.53ms
step:757/1385 train_time:94524ms step_avg:126.54ms
step:758/1385 train_time:94654ms step_avg:126.54ms
step:759/1385 train_time:94786ms step_avg:126.55ms
step:760/1385 train_time:94919ms step_avg:126.56ms
step:761/1385 train_time:95051ms step_avg:126.57ms
step:762/1385 train_time:95182ms step_avg:126.57ms
step:763/1385 train_time:95314ms step_avg:126.58ms
step:764/1385 train_time:95446ms step_avg:126.59ms
step:765/1385 train_time:95575ms step_avg:126.59ms
step:766/1385 train_time:95706ms step_avg:126.59ms
step:767/1385 train_time:95838ms step_avg:126.60ms
step:768/1385 train_time:95969ms step_avg:126.61ms
step:769/1385 train_time:96101ms step_avg:126.61ms
step:770/1385 train_time:96233ms step_avg:126.62ms
step:771/1385 train_time:96365ms step_avg:126.63ms
step:772/1385 train_time:96496ms step_avg:126.64ms
step:773/1385 train_time:96627ms step_avg:126.64ms
step:774/1385 train_time:96757ms step_avg:126.65ms
step:775/1385 train_time:96888ms step_avg:126.65ms
step:776/1385 train_time:97019ms step_avg:126.66ms
step:777/1385 train_time:97151ms step_avg:126.66ms
step:778/1385 train_time:97282ms step_avg:126.67ms
step:779/1385 train_time:97414ms step_avg:126.68ms
step:780/1385 train_time:97545ms step_avg:126.68ms
step:781/1385 train_time:97676ms step_avg:126.69ms
step:782/1385 train_time:97808ms step_avg:126.69ms
step:783/1385 train_time:97939ms step_avg:126.70ms
step:784/1385 train_time:98069ms step_avg:126.70ms
step:785/1385 train_time:98200ms step_avg:126.71ms
step:786/1385 train_time:98331ms step_avg:126.72ms
step:787/1385 train_time:98463ms step_avg:126.72ms
step:788/1385 train_time:98593ms step_avg:126.73ms
step:789/1385 train_time:98725ms step_avg:126.73ms
step:790/1385 train_time:98856ms step_avg:126.74ms
step:791/1385 train_time:98987ms step_avg:126.74ms
step:792/1385 train_time:99119ms step_avg:126.75ms
step:793/1385 train_time:99250ms step_avg:126.76ms
step:794/1385 train_time:99381ms step_avg:126.76ms
step:795/1385 train_time:99514ms step_avg:126.77ms
step:796/1385 train_time:99645ms step_avg:126.77ms
step:797/1385 train_time:99776ms step_avg:126.78ms
step:798/1385 train_time:99908ms step_avg:126.79ms
step:799/1385 train_time:100041ms step_avg:126.79ms
step:800/1385 train_time:100172ms step_avg:126.80ms
step:801/1385 train_time:100302ms step_avg:126.80ms
step:802/1385 train_time:100434ms step_avg:126.81ms
step:803/1385 train_time:100566ms step_avg:126.82ms
step:804/1385 train_time:100695ms step_avg:126.82ms
step:805/1385 train_time:100827ms step_avg:126.83ms
step:806/1385 train_time:100958ms step_avg:126.83ms
step:807/1385 train_time:101088ms step_avg:126.84ms
step:808/1385 train_time:101220ms step_avg:126.84ms
step:809/1385 train_time:101351ms step_avg:126.85ms
step:810/1385 train_time:101483ms step_avg:126.85ms
step:811/1385 train_time:101615ms step_avg:126.86ms
step:812/1385 train_time:101746ms step_avg:126.87ms
step:813/1385 train_time:101877ms step_avg:126.87ms
step:814/1385 train_time:102009ms step_avg:126.88ms
step:815/1385 train_time:102140ms step_avg:126.88ms
step:816/1385 train_time:102271ms step_avg:126.89ms
step:817/1385 train_time:102402ms step_avg:126.89ms
step:818/1385 train_time:102532ms step_avg:126.90ms
step:819/1385 train_time:102664ms step_avg:126.90ms
step:820/1385 train_time:102796ms step_avg:126.91ms
step:821/1385 train_time:102927ms step_avg:126.91ms
step:822/1385 train_time:103059ms step_avg:126.92ms
step:823/1385 train_time:103189ms step_avg:126.92ms
step:824/1385 train_time:103321ms step_avg:126.93ms
step:825/1385 train_time:103452ms step_avg:126.93ms
step:826/1385 train_time:103585ms step_avg:126.94ms
step:827/1385 train_time:103717ms step_avg:126.95ms
step:828/1385 train_time:103848ms step_avg:126.95ms
step:829/1385 train_time:103982ms step_avg:126.96ms
step:830/1385 train_time:104112ms step_avg:126.97ms
step:831/1385 train_time:104244ms step_avg:126.97ms
step:832/1385 train_time:104377ms step_avg:126.98ms
step:833/1385 train_time:104508ms step_avg:126.98ms
step:834/1385 train_time:104641ms step_avg:126.99ms
step:835/1385 train_time:104772ms step_avg:127.00ms
step:836/1385 train_time:104903ms step_avg:127.00ms
step:837/1385 train_time:105035ms step_avg:127.01ms
step:838/1385 train_time:105166ms step_avg:127.01ms
step:839/1385 train_time:105298ms step_avg:127.02ms
step:840/1385 train_time:105429ms step_avg:127.02ms
step:841/1385 train_time:105561ms step_avg:127.03ms
step:842/1385 train_time:105694ms step_avg:127.04ms
step:843/1385 train_time:105824ms step_avg:127.04ms
step:844/1385 train_time:105954ms step_avg:127.04ms
step:845/1385 train_time:106086ms step_avg:127.05ms
step:846/1385 train_time:106218ms step_avg:127.05ms
step:847/1385 train_time:106349ms step_avg:127.06ms
step:848/1385 train_time:106481ms step_avg:127.07ms
step:849/1385 train_time:106613ms step_avg:127.07ms
step:850/1385 train_time:106744ms step_avg:127.08ms
step:851/1385 train_time:106877ms step_avg:127.08ms
step:852/1385 train_time:107009ms step_avg:127.09ms
step:853/1385 train_time:107141ms step_avg:127.09ms
step:854/1385 train_time:107271ms step_avg:127.10ms
step:855/1385 train_time:107402ms step_avg:127.10ms
step:856/1385 train_time:107533ms step_avg:127.11ms
step:857/1385 train_time:107666ms step_avg:127.11ms
step:858/1385 train_time:107797ms step_avg:127.12ms
step:859/1385 train_time:107929ms step_avg:127.12ms
step:860/1385 train_time:108060ms step_avg:127.13ms
step:861/1385 train_time:108191ms step_avg:127.13ms
step:862/1385 train_time:108323ms step_avg:127.14ms
step:863/1385 train_time:108455ms step_avg:127.15ms
step:864/1385 train_time:108587ms step_avg:127.15ms
step:865/1385 train_time:108719ms step_avg:127.16ms
step:866/1385 train_time:108851ms step_avg:127.16ms
step:867/1385 train_time:108983ms step_avg:127.17ms
step:868/1385 train_time:109114ms step_avg:127.17ms
step:869/1385 train_time:109246ms step_avg:127.18ms
step:870/1385 train_time:109377ms step_avg:127.18ms
step:871/1385 train_time:109508ms step_avg:127.19ms
step:872/1385 train_time:109641ms step_avg:127.19ms
step:873/1385 train_time:109773ms step_avg:127.20ms
step:874/1385 train_time:109905ms step_avg:127.20ms
step:875/1385 train_time:110036ms step_avg:127.21ms
step:875/1385 val_loss:3.4747 train_time:110167ms step_avg:127.36ms
step:876/1385 train_time:110188ms step_avg:127.24ms
step:877/1385 train_time:110312ms step_avg:127.23ms
step:878/1385 train_time:110446ms step_avg:127.24ms
step:879/1385 train_time:110578ms step_avg:127.25ms
step:880/1385 train_time:110709ms step_avg:127.25ms
step:881/1385 train_time:110840ms step_avg:127.26ms
step:882/1385 train_time:110972ms step_avg:127.26ms
step:883/1385 train_time:111103ms step_avg:127.27ms
step:884/1385 train_time:111235ms step_avg:127.27ms
step:885/1385 train_time:111368ms step_avg:127.28ms
step:886/1385 train_time:111501ms step_avg:127.28ms
step:887/1385 train_time:111633ms step_avg:127.29ms
step:888/1385 train_time:111764ms step_avg:127.29ms
step:889/1385 train_time:111897ms step_avg:127.30ms
step:890/1385 train_time:112027ms step_avg:127.30ms
step:891/1385 train_time:112158ms step_avg:127.31ms
step:892/1385 train_time:112290ms step_avg:127.31ms
step:893/1385 train_time:112423ms step_avg:127.32ms
step:894/1385 train_time:112553ms step_avg:127.32ms
step:895/1385 train_time:112685ms step_avg:127.33ms
step:896/1385 train_time:112817ms step_avg:127.33ms
step:897/1385 train_time:112949ms step_avg:127.34ms
step:898/1385 train_time:113079ms step_avg:127.34ms
step:899/1385 train_time:113212ms step_avg:127.35ms
step:900/1385 train_time:113343ms step_avg:127.35ms
step:901/1385 train_time:113474ms step_avg:127.36ms
step:902/1385 train_time:113606ms step_avg:127.36ms
step:903/1385 train_time:113738ms step_avg:127.37ms
step:904/1385 train_time:113870ms step_avg:127.37ms
step:905/1385 train_time:114001ms step_avg:127.38ms
step:906/1385 train_time:114133ms step_avg:127.38ms
step:907/1385 train_time:114267ms step_avg:127.39ms
step:908/1385 train_time:114398ms step_avg:127.39ms
step:909/1385 train_time:114530ms step_avg:127.40ms
step:910/1385 train_time:114663ms step_avg:127.40ms
step:911/1385 train_time:114796ms step_avg:127.41ms
step:912/1385 train_time:114927ms step_avg:127.41ms
step:913/1385 train_time:115059ms step_avg:127.42ms
step:914/1385 train_time:115190ms step_avg:127.42ms
step:915/1385 train_time:115323ms step_avg:127.43ms
step:916/1385 train_time:115454ms step_avg:127.43ms
step:917/1385 train_time:115585ms step_avg:127.44ms
step:918/1385 train_time:115716ms step_avg:127.44ms
step:919/1385 train_time:115850ms step_avg:127.45ms
step:920/1385 train_time:115981ms step_avg:127.45ms
step:921/1385 train_time:116113ms step_avg:127.46ms
step:922/1385 train_time:116245ms step_avg:127.46ms
step:923/1385 train_time:116376ms step_avg:127.47ms
step:924/1385 train_time:116507ms step_avg:127.47ms
step:925/1385 train_time:116639ms step_avg:127.47ms
step:926/1385 train_time:116772ms step_avg:127.48ms
step:927/1385 train_time:116906ms step_avg:127.49ms
step:928/1385 train_time:117038ms step_avg:127.49ms
step:929/1385 train_time:117172ms step_avg:127.50ms
step:930/1385 train_time:117304ms step_avg:127.50ms
step:931/1385 train_time:117438ms step_avg:127.51ms
step:932/1385 train_time:117571ms step_avg:127.52ms
step:933/1385 train_time:117703ms step_avg:127.52ms
step:934/1385 train_time:117836ms step_avg:127.53ms
step:935/1385 train_time:117970ms step_avg:127.54ms
step:936/1385 train_time:118103ms step_avg:127.54ms
step:937/1385 train_time:118236ms step_avg:127.55ms
step:938/1385 train_time:118371ms step_avg:127.55ms
step:939/1385 train_time:118504ms step_avg:127.56ms
step:940/1385 train_time:118638ms step_avg:127.57ms
step:941/1385 train_time:118771ms step_avg:127.57ms
step:942/1385 train_time:118904ms step_avg:127.58ms
step:943/1385 train_time:119038ms step_avg:127.59ms
step:944/1385 train_time:119172ms step_avg:127.59ms
step:945/1385 train_time:119306ms step_avg:127.60ms
step:946/1385 train_time:119438ms step_avg:127.61ms
step:947/1385 train_time:119571ms step_avg:127.61ms
step:948/1385 train_time:119706ms step_avg:127.62ms
step:949/1385 train_time:119839ms step_avg:127.62ms
step:950/1385 train_time:119973ms step_avg:127.63ms
step:951/1385 train_time:120107ms step_avg:127.64ms
step:952/1385 train_time:120239ms step_avg:127.64ms
step:953/1385 train_time:120373ms step_avg:127.65ms
step:954/1385 train_time:120505ms step_avg:127.65ms
step:955/1385 train_time:120639ms step_avg:127.66ms
step:956/1385 train_time:120772ms step_avg:127.67ms
step:957/1385 train_time:120906ms step_avg:127.67ms
step:958/1385 train_time:121040ms step_avg:127.68ms
step:959/1385 train_time:121173ms step_avg:127.68ms
step:960/1385 train_time:121307ms step_avg:127.69ms
step:961/1385 train_time:121440ms step_avg:127.70ms
step:962/1385 train_time:121573ms step_avg:127.70ms
step:963/1385 train_time:121706ms step_avg:127.71ms
step:964/1385 train_time:121839ms step_avg:127.71ms
step:965/1385 train_time:121972ms step_avg:127.72ms
step:966/1385 train_time:122105ms step_avg:127.72ms
step:967/1385 train_time:122238ms step_avg:127.73ms
step:968/1385 train_time:122371ms step_avg:127.74ms
step:969/1385 train_time:122505ms step_avg:127.74ms
step:970/1385 train_time:122637ms step_avg:127.75ms
step:971/1385 train_time:122772ms step_avg:127.75ms
step:972/1385 train_time:122905ms step_avg:127.76ms
step:973/1385 train_time:123039ms step_avg:127.77ms
step:974/1385 train_time:123173ms step_avg:127.77ms
step:975/1385 train_time:123306ms step_avg:127.78ms
step:976/1385 train_time:123439ms step_avg:127.78ms
step:977/1385 train_time:123572ms step_avg:127.79ms
step:978/1385 train_time:123704ms step_avg:127.79ms
step:979/1385 train_time:123837ms step_avg:127.80ms
step:980/1385 train_time:123970ms step_avg:127.80ms
step:981/1385 train_time:124103ms step_avg:127.81ms
step:982/1385 train_time:124235ms step_avg:127.81ms
step:983/1385 train_time:124369ms step_avg:127.82ms
step:984/1385 train_time:124502ms step_avg:127.83ms
step:985/1385 train_time:124636ms step_avg:127.83ms
step:986/1385 train_time:124771ms step_avg:127.84ms
step:987/1385 train_time:124903ms step_avg:127.84ms
step:988/1385 train_time:125035ms step_avg:127.85ms
step:989/1385 train_time:125168ms step_avg:127.85ms
step:990/1385 train_time:125303ms step_avg:127.86ms
step:991/1385 train_time:125435ms step_avg:127.86ms
step:992/1385 train_time:125568ms step_avg:127.87ms
step:993/1385 train_time:125703ms step_avg:127.88ms
step:994/1385 train_time:125835ms step_avg:127.88ms
step:995/1385 train_time:125968ms step_avg:127.89ms
step:996/1385 train_time:126102ms step_avg:127.89ms
step:997/1385 train_time:126234ms step_avg:127.90ms
step:998/1385 train_time:126368ms step_avg:127.90ms
step:999/1385 train_time:126502ms step_avg:127.91ms
step:1000/1385 train_time:126635ms step_avg:127.91ms
step:1000/1385 val_loss:3.4125 train_time:126768ms step_avg:128.05ms
step:1001/1385 train_time:126788ms step_avg:127.94ms
step:1002/1385 train_time:126914ms step_avg:127.94ms
step:1003/1385 train_time:127051ms step_avg:127.95ms
step:1004/1385 train_time:127183ms step_avg:127.95ms
step:1005/1385 train_time:127317ms step_avg:127.96ms
step:1006/1385 train_time:127449ms step_avg:127.96ms
step:1007/1385 train_time:127581ms step_avg:127.97ms
step:1008/1385 train_time:127715ms step_avg:127.97ms
step:1009/1385 train_time:127848ms step_avg:127.98ms
step:1010/1385 train_time:127982ms step_avg:127.98ms
step:1011/1385 train_time:128119ms step_avg:127.99ms
step:1012/1385 train_time:128252ms step_avg:128.00ms
step:1013/1385 train_time:128385ms step_avg:128.00ms
step:1014/1385 train_time:128518ms step_avg:128.01ms
step:1015/1385 train_time:128651ms step_avg:128.01ms
step:1016/1385 train_time:128785ms step_avg:128.02ms
step:1017/1385 train_time:128918ms step_avg:128.02ms
step:1018/1385 train_time:129051ms step_avg:128.03ms
step:1019/1385 train_time:129184ms step_avg:128.03ms
step:1020/1385 train_time:129317ms step_avg:128.04ms
step:1021/1385 train_time:129450ms step_avg:128.04ms
step:1022/1385 train_time:129583ms step_avg:128.05ms
step:1023/1385 train_time:129717ms step_avg:128.05ms
step:1024/1385 train_time:129850ms step_avg:128.06ms
step:1025/1385 train_time:129984ms step_avg:128.06ms
step:1026/1385 train_time:130120ms step_avg:128.07ms
step:1027/1385 train_time:130253ms step_avg:128.08ms
step:1028/1385 train_time:130386ms step_avg:128.08ms
step:1029/1385 train_time:130521ms step_avg:128.09ms
step:1030/1385 train_time:130656ms step_avg:128.09ms
step:1031/1385 train_time:130790ms step_avg:128.10ms
step:1032/1385 train_time:130923ms step_avg:128.11ms
step:1033/1385 train_time:131056ms step_avg:128.11ms
step:1034/1385 train_time:131191ms step_avg:128.12ms
step:1035/1385 train_time:131325ms step_avg:128.12ms
step:1036/1385 train_time:131458ms step_avg:128.13ms
step:1037/1385 train_time:131592ms step_avg:128.13ms
step:1038/1385 train_time:131726ms step_avg:128.14ms
step:1039/1385 train_time:131857ms step_avg:128.14ms
step:1040/1385 train_time:131991ms step_avg:128.15ms
step:1041/1385 train_time:132125ms step_avg:128.15ms
step:1042/1385 train_time:132258ms step_avg:128.16ms
step:1043/1385 train_time:132391ms step_avg:128.16ms
step:1044/1385 train_time:132525ms step_avg:128.17ms
step:1045/1385 train_time:132658ms step_avg:128.17ms
step:1046/1385 train_time:132793ms step_avg:128.18ms
step:1047/1385 train_time:132925ms step_avg:128.18ms
step:1048/1385 train_time:133059ms step_avg:128.19ms
step:1049/1385 train_time:133194ms step_avg:128.19ms
step:1050/1385 train_time:133328ms step_avg:128.20ms
step:1051/1385 train_time:133463ms step_avg:128.21ms
step:1052/1385 train_time:133596ms step_avg:128.21ms
step:1053/1385 train_time:133729ms step_avg:128.22ms
step:1054/1385 train_time:133862ms step_avg:128.22ms
step:1055/1385 train_time:133995ms step_avg:128.22ms
step:1056/1385 train_time:134129ms step_avg:128.23ms
step:1057/1385 train_time:134263ms step_avg:128.24ms
step:1058/1385 train_time:134397ms step_avg:128.24ms
step:1059/1385 train_time:134531ms step_avg:128.25ms
step:1060/1385 train_time:134664ms step_avg:128.25ms
step:1061/1385 train_time:134797ms step_avg:128.26ms
step:1062/1385 train_time:134930ms step_avg:128.26ms
step:1063/1385 train_time:135064ms step_avg:128.27ms
step:1064/1385 train_time:135198ms step_avg:128.27ms
step:1065/1385 train_time:135331ms step_avg:128.28ms
step:1066/1385 train_time:135465ms step_avg:128.28ms
step:1067/1385 train_time:135598ms step_avg:128.29ms
step:1068/1385 train_time:135730ms step_avg:128.29ms
step:1069/1385 train_time:135864ms step_avg:128.29ms
step:1070/1385 train_time:135997ms step_avg:128.30ms
step:1071/1385 train_time:136135ms step_avg:128.31ms
step:1072/1385 train_time:136267ms step_avg:128.31ms
step:1073/1385 train_time:136399ms step_avg:128.31ms
step:1074/1385 train_time:136532ms step_avg:128.32ms
step:1075/1385 train_time:136666ms step_avg:128.32ms
step:1076/1385 train_time:136798ms step_avg:128.33ms
step:1077/1385 train_time:136932ms step_avg:128.33ms
step:1078/1385 train_time:137066ms step_avg:128.34ms
step:1079/1385 train_time:137202ms step_avg:128.35ms
step:1080/1385 train_time:137336ms step_avg:128.35ms
step:1081/1385 train_time:137470ms step_avg:128.36ms
step:1082/1385 train_time:137602ms step_avg:128.36ms
step:1083/1385 train_time:137735ms step_avg:128.36ms
step:1084/1385 train_time:137869ms step_avg:128.37ms
step:1085/1385 train_time:138003ms step_avg:128.37ms
step:1086/1385 train_time:138136ms step_avg:128.38ms
step:1087/1385 train_time:138271ms step_avg:128.39ms
step:1088/1385 train_time:138405ms step_avg:128.39ms
step:1089/1385 train_time:138539ms step_avg:128.40ms
step:1090/1385 train_time:138674ms step_avg:128.40ms
step:1091/1385 train_time:138808ms step_avg:128.41ms
step:1092/1385 train_time:138942ms step_avg:128.41ms
step:1093/1385 train_time:139075ms step_avg:128.42ms
step:1094/1385 train_time:139209ms step_avg:128.42ms
step:1095/1385 train_time:139344ms step_avg:128.43ms
step:1096/1385 train_time:139478ms step_avg:128.43ms
step:1097/1385 train_time:139611ms step_avg:128.44ms
step:1098/1385 train_time:139746ms step_avg:128.44ms
step:1099/1385 train_time:139878ms step_avg:128.45ms
step:1100/1385 train_time:140012ms step_avg:128.45ms
step:1101/1385 train_time:140145ms step_avg:128.46ms
step:1102/1385 train_time:140279ms step_avg:128.46ms
step:1103/1385 train_time:140413ms step_avg:128.47ms
step:1104/1385 train_time:140547ms step_avg:128.47ms
step:1105/1385 train_time:140681ms step_avg:128.48ms
step:1106/1385 train_time:140813ms step_avg:128.48ms
step:1107/1385 train_time:140948ms step_avg:128.48ms
step:1108/1385 train_time:141082ms step_avg:128.49ms
step:1109/1385 train_time:141216ms step_avg:128.49ms
step:1110/1385 train_time:141350ms step_avg:128.50ms
step:1111/1385 train_time:141484ms step_avg:128.50ms
step:1112/1385 train_time:141618ms step_avg:128.51ms
step:1113/1385 train_time:141751ms step_avg:128.51ms
step:1114/1385 train_time:141884ms step_avg:128.52ms
step:1115/1385 train_time:142017ms step_avg:128.52ms
step:1116/1385 train_time:142150ms step_avg:128.53ms
step:1117/1385 train_time:142283ms step_avg:128.53ms
step:1118/1385 train_time:142419ms step_avg:128.54ms
step:1119/1385 train_time:142552ms step_avg:128.54ms
step:1120/1385 train_time:142684ms step_avg:128.54ms
step:1121/1385 train_time:142818ms step_avg:128.55ms
step:1122/1385 train_time:142951ms step_avg:128.55ms
step:1123/1385 train_time:143083ms step_avg:128.56ms
step:1124/1385 train_time:143219ms step_avg:128.56ms
step:1125/1385 train_time:143351ms step_avg:128.57ms
step:1125/1385 val_loss:3.3617 train_time:143483ms step_avg:128.68ms
step:1126/1385 train_time:143504ms step_avg:128.59ms
step:1127/1385 train_time:143628ms step_avg:128.58ms
step:1128/1385 train_time:143763ms step_avg:128.59ms
step:1129/1385 train_time:143897ms step_avg:128.59ms
step:1130/1385 train_time:144028ms step_avg:128.60ms
step:1131/1385 train_time:144163ms step_avg:128.60ms
step:1132/1385 train_time:144297ms step_avg:128.61ms
step:1133/1385 train_time:144430ms step_avg:128.61ms
step:1134/1385 train_time:144565ms step_avg:128.62ms
step:1135/1385 train_time:144703ms step_avg:128.62ms
step:1136/1385 train_time:144842ms step_avg:128.63ms
step:1137/1385 train_time:144976ms step_avg:128.64ms
step:1138/1385 train_time:145113ms step_avg:128.65ms
step:1139/1385 train_time:145247ms step_avg:128.65ms
step:1140/1385 train_time:145382ms step_avg:128.66ms
step:1141/1385 train_time:145517ms step_avg:128.66ms
step:1142/1385 train_time:145650ms step_avg:128.67ms
step:1143/1385 train_time:145788ms step_avg:128.67ms
step:1144/1385 train_time:145921ms step_avg:128.68ms
step:1145/1385 train_time:146056ms step_avg:128.68ms
step:1146/1385 train_time:146191ms step_avg:128.69ms
step:1147/1385 train_time:146326ms step_avg:128.69ms
step:1148/1385 train_time:146462ms step_avg:128.70ms
step:1149/1385 train_time:146597ms step_avg:128.71ms
step:1150/1385 train_time:146732ms step_avg:128.71ms
step:1151/1385 train_time:146869ms step_avg:128.72ms
step:1152/1385 train_time:147003ms step_avg:128.72ms
step:1153/1385 train_time:147138ms step_avg:128.73ms
step:1154/1385 train_time:147273ms step_avg:128.73ms
step:1155/1385 train_time:147408ms step_avg:128.74ms
step:1156/1385 train_time:147545ms step_avg:128.75ms
step:1157/1385 train_time:147680ms step_avg:128.75ms
step:1158/1385 train_time:147814ms step_avg:128.76ms
step:1159/1385 train_time:147949ms step_avg:128.76ms
step:1160/1385 train_time:148083ms step_avg:128.77ms
step:1161/1385 train_time:148217ms step_avg:128.77ms
step:1162/1385 train_time:148352ms step_avg:128.78ms
step:1163/1385 train_time:148486ms step_avg:128.78ms
step:1164/1385 train_time:148621ms step_avg:128.79ms
step:1165/1385 train_time:148756ms step_avg:128.79ms
step:1166/1385 train_time:148889ms step_avg:128.80ms
step:1167/1385 train_time:149023ms step_avg:128.80ms
step:1168/1385 train_time:149159ms step_avg:128.81ms
step:1169/1385 train_time:149295ms step_avg:128.81ms
step:1170/1385 train_time:149429ms step_avg:128.82ms
step:1171/1385 train_time:149565ms step_avg:128.82ms
step:1172/1385 train_time:149700ms step_avg:128.83ms
step:1173/1385 train_time:149835ms step_avg:128.83ms
step:1174/1385 train_time:149975ms step_avg:128.84ms
step:1175/1385 train_time:150111ms step_avg:128.85ms
step:1176/1385 train_time:150247ms step_avg:128.86ms
step:1177/1385 train_time:150384ms step_avg:128.86ms
step:1178/1385 train_time:150521ms step_avg:128.87ms
step:1179/1385 train_time:150654ms step_avg:128.87ms
step:1180/1385 train_time:150790ms step_avg:128.88ms
step:1181/1385 train_time:150925ms step_avg:128.89ms
step:1182/1385 train_time:151059ms step_avg:128.89ms
step:1183/1385 train_time:151195ms step_avg:128.90ms
step:1184/1385 train_time:151331ms step_avg:128.90ms
step:1185/1385 train_time:151468ms step_avg:128.91ms
step:1186/1385 train_time:151601ms step_avg:128.91ms
step:1187/1385 train_time:151743ms step_avg:128.92ms
step:1188/1385 train_time:151877ms step_avg:128.93ms
step:1189/1385 train_time:152012ms step_avg:128.93ms
step:1190/1385 train_time:152147ms step_avg:128.94ms
step:1191/1385 train_time:152281ms step_avg:128.94ms
step:1192/1385 train_time:152419ms step_avg:128.95ms
step:1193/1385 train_time:152553ms step_avg:128.95ms
step:1194/1385 train_time:152688ms step_avg:128.96ms
step:1195/1385 train_time:152824ms step_avg:128.97ms
step:1196/1385 train_time:152958ms step_avg:128.97ms
step:1197/1385 train_time:153094ms step_avg:128.98ms
step:1198/1385 train_time:153231ms step_avg:128.98ms
step:1199/1385 train_time:153365ms step_avg:128.99ms
step:1200/1385 train_time:153501ms step_avg:128.99ms
step:1201/1385 train_time:153634ms step_avg:129.00ms
step:1202/1385 train_time:153774ms step_avg:129.00ms
step:1203/1385 train_time:153912ms step_avg:129.01ms
step:1204/1385 train_time:154048ms step_avg:129.02ms
step:1205/1385 train_time:154183ms step_avg:129.02ms
step:1206/1385 train_time:154318ms step_avg:129.03ms
step:1207/1385 train_time:154452ms step_avg:129.03ms
step:1208/1385 train_time:154588ms step_avg:129.04ms
step:1209/1385 train_time:154725ms step_avg:129.05ms
step:1210/1385 train_time:154862ms step_avg:129.05ms
step:1211/1385 train_time:154997ms step_avg:129.06ms
step:1212/1385 train_time:155132ms step_avg:129.06ms
step:1213/1385 train_time:155266ms step_avg:129.07ms
step:1214/1385 train_time:155401ms step_avg:129.07ms
step:1215/1385 train_time:155537ms step_avg:129.08ms
step:1216/1385 train_time:155670ms step_avg:129.08ms
step:1217/1385 train_time:155805ms step_avg:129.08ms
step:1218/1385 train_time:155939ms step_avg:129.09ms
step:1219/1385 train_time:156072ms step_avg:129.09ms
step:1220/1385 train_time:156208ms step_avg:129.10ms
step:1221/1385 train_time:156342ms step_avg:129.10ms
step:1222/1385 train_time:156477ms step_avg:129.11ms
step:1223/1385 train_time:156612ms step_avg:129.11ms
step:1224/1385 train_time:156748ms step_avg:129.12ms
step:1225/1385 train_time:156885ms step_avg:129.12ms
step:1226/1385 train_time:157020ms step_avg:129.13ms
step:1227/1385 train_time:157154ms step_avg:129.13ms
step:1228/1385 train_time:157288ms step_avg:129.14ms
step:1229/1385 train_time:157423ms step_avg:129.14ms
step:1230/1385 train_time:157559ms step_avg:129.15ms
step:1231/1385 train_time:157693ms step_avg:129.15ms
step:1232/1385 train_time:157828ms step_avg:129.16ms
step:1233/1385 train_time:157963ms step_avg:129.16ms
step:1234/1385 train_time:158100ms step_avg:129.17ms
step:1235/1385 train_time:158235ms step_avg:129.17ms
step:1236/1385 train_time:158369ms step_avg:129.17ms
step:1237/1385 train_time:158503ms step_avg:129.18ms
step:1238/1385 train_time:158644ms step_avg:129.19ms
step:1239/1385 train_time:158778ms step_avg:129.19ms
step:1240/1385 train_time:158915ms step_avg:129.20ms
step:1241/1385 train_time:159051ms step_avg:129.20ms
step:1242/1385 train_time:159187ms step_avg:129.21ms
step:1243/1385 train_time:159321ms step_avg:129.21ms
step:1244/1385 train_time:159457ms step_avg:129.22ms
step:1245/1385 train_time:159592ms step_avg:129.22ms
step:1246/1385 train_time:159727ms step_avg:129.23ms
step:1247/1385 train_time:159863ms step_avg:129.23ms
step:1248/1385 train_time:159998ms step_avg:129.24ms
step:1249/1385 train_time:160132ms step_avg:129.24ms
step:1250/1385 train_time:160266ms step_avg:129.25ms
step:1250/1385 val_loss:3.3147 train_time:160400ms step_avg:129.35ms
step:1251/1385 train_time:160419ms step_avg:129.27ms
step:1252/1385 train_time:160545ms step_avg:129.26ms
step:1253/1385 train_time:160679ms step_avg:129.27ms
step:1254/1385 train_time:160812ms step_avg:129.27ms
step:1255/1385 train_time:160952ms step_avg:129.28ms
step:1256/1385 train_time:161086ms step_avg:129.28ms
step:1257/1385 train_time:161221ms step_avg:129.29ms
step:1258/1385 train_time:161355ms step_avg:129.29ms
step:1259/1385 train_time:161492ms step_avg:129.30ms
step:1260/1385 train_time:161628ms step_avg:129.30ms
step:1261/1385 train_time:161763ms step_avg:129.31ms
step:1262/1385 train_time:161899ms step_avg:129.31ms
step:1263/1385 train_time:162034ms step_avg:129.32ms
step:1264/1385 train_time:162168ms step_avg:129.32ms
step:1265/1385 train_time:162303ms step_avg:129.32ms
step:1266/1385 train_time:162440ms step_avg:129.33ms
step:1267/1385 train_time:162575ms step_avg:129.34ms
step:1268/1385 train_time:162710ms step_avg:129.34ms
step:1269/1385 train_time:162846ms step_avg:129.35ms
step:1270/1385 train_time:162981ms step_avg:129.35ms
step:1271/1385 train_time:163115ms step_avg:129.35ms
step:1272/1385 train_time:163248ms step_avg:129.36ms
step:1273/1385 train_time:163383ms step_avg:129.36ms
step:1274/1385 train_time:163517ms step_avg:129.36ms
step:1275/1385 train_time:163652ms step_avg:129.37ms
step:1276/1385 train_time:163787ms step_avg:129.37ms
step:1277/1385 train_time:163922ms step_avg:129.38ms
step:1278/1385 train_time:164057ms step_avg:129.38ms
step:1279/1385 train_time:164192ms step_avg:129.39ms
step:1280/1385 train_time:164330ms step_avg:129.39ms
step:1281/1385 train_time:164464ms step_avg:129.40ms
step:1282/1385 train_time:164599ms step_avg:129.40ms
step:1283/1385 train_time:164734ms step_avg:129.41ms
step:1284/1385 train_time:164869ms step_avg:129.41ms
step:1285/1385 train_time:165002ms step_avg:129.41ms
step:1286/1385 train_time:165137ms step_avg:129.42ms
step:1287/1385 train_time:165274ms step_avg:129.42ms
step:1288/1385 train_time:165410ms step_avg:129.43ms
step:1289/1385 train_time:165548ms step_avg:129.44ms
step:1290/1385 train_time:165685ms step_avg:129.44ms
step:1291/1385 train_time:165821ms step_avg:129.45ms
step:1292/1385 train_time:165958ms step_avg:129.45ms
step:1293/1385 train_time:166097ms step_avg:129.46ms
step:1294/1385 train_time:166233ms step_avg:129.46ms
step:1295/1385 train_time:166367ms step_avg:129.47ms
step:1296/1385 train_time:166501ms step_avg:129.47ms
step:1297/1385 train_time:166637ms step_avg:129.48ms
step:1298/1385 train_time:166772ms step_avg:129.48ms
step:1299/1385 train_time:166908ms step_avg:129.49ms
step:1300/1385 train_time:167043ms step_avg:129.49ms
step:1301/1385 train_time:167178ms step_avg:129.49ms
step:1302/1385 train_time:167313ms step_avg:129.50ms
step:1303/1385 train_time:167452ms step_avg:129.51ms
step:1304/1385 train_time:167588ms step_avg:129.51ms
step:1305/1385 train_time:167723ms step_avg:129.52ms
step:1306/1385 train_time:167857ms step_avg:129.52ms
step:1307/1385 train_time:167994ms step_avg:129.52ms
step:1308/1385 train_time:168129ms step_avg:129.53ms
step:1309/1385 train_time:168265ms step_avg:129.53ms
step:1310/1385 train_time:168401ms step_avg:129.54ms
step:1311/1385 train_time:168535ms step_avg:129.54ms
step:1312/1385 train_time:168670ms step_avg:129.55ms
step:1313/1385 train_time:168805ms step_avg:129.55ms
step:1314/1385 train_time:168939ms step_avg:129.55ms
step:1315/1385 train_time:169074ms step_avg:129.56ms
step:1316/1385 train_time:169208ms step_avg:129.56ms
step:1317/1385 train_time:169343ms step_avg:129.57ms
step:1318/1385 train_time:169479ms step_avg:129.57ms
step:1319/1385 train_time:169613ms step_avg:129.57ms
step:1320/1385 train_time:169748ms step_avg:129.58ms
step:1321/1385 train_time:169883ms step_avg:129.58ms
step:1322/1385 train_time:170022ms step_avg:129.59ms
step:1323/1385 train_time:170157ms step_avg:129.59ms
step:1324/1385 train_time:170292ms step_avg:129.60ms
step:1325/1385 train_time:170427ms step_avg:129.60ms
step:1326/1385 train_time:170561ms step_avg:129.61ms
step:1327/1385 train_time:170697ms step_avg:129.61ms
step:1328/1385 train_time:170832ms step_avg:129.61ms
step:1329/1385 train_time:170972ms step_avg:129.62ms
step:1330/1385 train_time:171107ms step_avg:129.63ms
step:1331/1385 train_time:171243ms step_avg:129.63ms
step:1332/1385 train_time:171382ms step_avg:129.64ms
step:1333/1385 train_time:171517ms step_avg:129.64ms
step:1334/1385 train_time:171652ms step_avg:129.65ms
step:1335/1385 train_time:171786ms step_avg:129.65ms
step:1336/1385 train_time:171924ms step_avg:129.66ms
step:1337/1385 train_time:172061ms step_avg:129.66ms
step:1338/1385 train_time:172199ms step_avg:129.67ms
step:1339/1385 train_time:172334ms step_avg:129.67ms
step:1340/1385 train_time:172472ms step_avg:129.68ms
step:1341/1385 train_time:172606ms step_avg:129.68ms
step:1342/1385 train_time:172742ms step_avg:129.69ms
step:1343/1385 train_time:172878ms step_avg:129.69ms
step:1344/1385 train_time:173014ms step_avg:129.70ms
step:1345/1385 train_time:173152ms step_avg:129.70ms
step:1346/1385 train_time:173286ms step_avg:129.71ms
step:1347/1385 train_time:173423ms step_avg:129.71ms
step:1348/1385 train_time:173558ms step_avg:129.71ms
step:1349/1385 train_time:173694ms step_avg:129.72ms
step:1350/1385 train_time:173830ms step_avg:129.72ms
step:1351/1385 train_time:173966ms step_avg:129.73ms
step:1352/1385 train_time:174104ms step_avg:129.73ms
step:1353/1385 train_time:174240ms step_avg:129.74ms
step:1354/1385 train_time:174376ms step_avg:129.74ms
step:1355/1385 train_time:174511ms step_avg:129.75ms
step:1356/1385 train_time:174646ms step_avg:129.75ms
step:1357/1385 train_time:174782ms step_avg:129.76ms
step:1358/1385 train_time:174918ms step_avg:129.76ms
step:1359/1385 train_time:175054ms step_avg:129.77ms
step:1360/1385 train_time:175192ms step_avg:129.77ms
step:1361/1385 train_time:175328ms step_avg:129.78ms
step:1362/1385 train_time:175465ms step_avg:129.78ms
step:1363/1385 train_time:175603ms step_avg:129.79ms
step:1364/1385 train_time:175739ms step_avg:129.79ms
step:1365/1385 train_time:175873ms step_avg:129.80ms
step:1366/1385 train_time:176009ms step_avg:129.80ms
step:1367/1385 train_time:176146ms step_avg:129.81ms
step:1368/1385 train_time:176282ms step_avg:129.81ms
step:1369/1385 train_time:176421ms step_avg:129.82ms
step:1370/1385 train_time:176561ms step_avg:129.82ms
step:1371/1385 train_time:176698ms step_avg:129.83ms
step:1372/1385 train_time:176837ms step_avg:129.84ms
step:1373/1385 train_time:176972ms step_avg:129.84ms
step:1374/1385 train_time:177111ms step_avg:129.85ms
step:1375/1385 train_time:177246ms step_avg:129.85ms
step:1375/1385 val_loss:3.2811 train_time:177379ms step_avg:129.95ms
step:1376/1385 train_time:177400ms step_avg:129.87ms
step:1377/1385 train_time:177525ms step_avg:129.86ms
step:1378/1385 train_time:177660ms step_avg:129.87ms
step:1379/1385 train_time:177796ms step_avg:129.87ms
step:1380/1385 train_time:177932ms step_avg:129.88ms
step:1381/1385 train_time:178069ms step_avg:129.88ms
step:1382/1385 train_time:178206ms step_avg:129.89ms
step:1383/1385 train_time:178341ms step_avg:129.89ms
step:1384/1385 train_time:178480ms step_avg:129.90ms
step:1385/1385 train_time:178616ms step_avg:129.90ms
step:1385/1385 val_loss:3.2792 train_time:178752ms step_avg:130.00ms
peak memory allocated: 37653 MiB reserved: 39236 MiB
